---
title : Domain Adaptive Inference for Neural Machine Translation

---

## Abstract

We investigate adaptive ensemble weighting for Neural Machine Translation, addressing the case of improving performance on a new and potentially unknown domain without sacrificing performance on the original domain. We adapt sequentially across two SpanishEnglish and three English-German tasks, comparing unregularized fine-tuning, L2 and Elastic Weight Consolidation. We then report a novel scheme for adaptive NMT ensemble decoding by extending Bayesian Interpolation with source information, and show strong improvements across test domains without access to the domain label.

## Notes

In NMT fine-tuning, a model is first trained on a task A, typically translating a large generaldomain corpus and fine-tuned on task B, a new domain. Without regularization, catastrophic forgetting can occur: performance on task A degrades as parameters adjust to the new objective. 

$$L(\theta) = L_B (\theta) + \Lambda \sum_j F_j (\theta_j âˆ’ \theta \times A_j^{*} )^2$$

$$argmax_y p(y \vert x) = argmax_y \sum_{t=1}^{T} p(t, y \vert x)$$

$$ \rightarrow p(y_i \vert h_i , x) = \sum_{t=1}^{T} p(t, y_i \vert h_i, x)$$
