---
title : Revisiting Low-Resource Neural Machine Translation: A case study

---

Figure 1 reproduces a plot by Koehn and Knowles (2017) which shows that their NMT system only outperforms their PBSMT system when more than 100 million words (approx. 5 million sentences) of parallel training data are available. Results shown by Lample et al. (2018b) are similar, showing that unsupervised NMT outperforms supervised systems if few parallel resources are available. In both papers, NMT systems are trained with hyperparameters that are typical for high-resource settings, and the authors did not tune hyperparameters, or change network architectures, to optimize NMT for low-resource conditions.

Methods to improve NMT with monolingual data range from the integration of a separately trained language model to the training of parts of the NMT model with additional objectives, including a language modelling objective, an autoencoding objective, or a round-trip objective, where the model is trained to predict monolingual (target-side) training data that has been back-translated into the source language.

For high-resource settings, the effect of vocabulary size on translation quality is relatively small; Haddow et al. (2018) report mixed results when comparing vocabularies of 30k and 90k subwords. In low-resource settings, large vocabularies result in low-frequency (sub)words being represented as atomic units at training time, and the ability to learn good high-dimensional representations of these is doubtful. Sennrich et al. (2017a) propose a minimum frequency threshold for subword units, and splitting any less frequent subword into smaller units or characters.