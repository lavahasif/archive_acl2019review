---
title : Bilingual Lexicon Induction with Semi-supervision in Non-Isometric Embedding Spaces

---

Bilingual lexicon induction (BLI), the task of finding corresponding words in two languages from comparable corpora. Most work on BLI uses methods that learn a mapping between two word embedding spaces which makes it possible to leverage pre-trained embeddings learned on large monolingual corpora. A commonly used method for BLI, which is also empirically effective, involves learning an orthogonal mapping between the two embedding spaces. However, learning an orthogonal mapping inherently assumes that the embedding spaces for the two languages are isometric (subsequently referred to as the orthogonality assumption). This is a particularly strong assumption that may not necessarily hold true, and consequently we can expect methods relying on this assumption to provide sub-optimal results.

Weak Orthogonality Constraint: Given an embedding space $$\chi$$, we define a consistency loss that maximizes a similarity function fa between x and $$W^T W x$$, $$x \in \chi$$. This cyclic consistency loss $$L_{W \vert O}$$ encourages orthogonality of the W matrix based on the joint optimization:

$$ L_{W \vert O} = \frac{1}{\vert \chi \vert} \sum_{x_i \in \chi} f_a(x_i, W^T W x_i)$$

TODO: [Normalized Word Embedding and Orthogonal Transform for Bilingual Word Translation](https://pdfs.semanticscholar.org/77e5/76c02792d7df5b102bb81d49df4b5382e1cc.pdf) 에 word embedding space alignment 하는 내용이 있으니 이것도 리뷰해 볼 것

TODO: [Word translation without parallel data](https://openreview.net/forum?id=H196sainb) 에 parallel dictionary induction 에 대한 내용이 있으니 참고해 볼 것.